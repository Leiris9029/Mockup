# 2026 R&D AI Co-scientist PRD

# One Pager

## Overview

본 프로덕트는 EEG/PSG raw 신호와 이에 페어된 임상 데이터, 그리고 연구 질문을 입력으로 받아, 연구를 end-to-end로 수행하는 연구용 플랫폼이다. 본 시스템은 (1) 연구 질문을 Task specification으로 구조화하고, (2) 데이터 적합성·품질·코호트/라벨 정의 리스크를 점검하며, (3) 공개 모델 활용/논문 기반 재현/신규 모델 개발 중 최적 전략을 선택해 학습·검증·외부검증을 수행한다. 이후 (4) EEG/PSG 특화 xAI(time–channel–frequency, 사례 기반 오류 분석)를 통해 결과의 근거를 제공하고, (5) LLM을 결합해 결과 요약, 임상적 함의, 한계 및 추가 분석 제안을 근거 기반으로 생성하며, Methods/Results/Limitations 중심의 IRB/논문 초안까지 산출한다. 이 플랫폼은 단순한 모델 자동화가 아니라, 연구 수행 공정을 표준화하여 인하우스 연구 생산성을 “제품화 가능한 역량”으로 전환하는 것을 목표로 하며, 장기적으로 임상(doctor assistant) 및 제약사 clinical trial/BD&L의 co-scientist로 확장 가능한 기반을 구축한다.

## Problem

### P1. 연구 수요(태스크) 증가 대비 공급(분석 리소스) 한계

- 인하우스 EEG/PSG 기반 딥러닝 연구 태스크(예: **약물 반응성 예측, 예후 예측, 진단/감별, 스크리닝**)가 이미 누적되어 있고, 신규 IRB/데이터 확보로 지속 증가 중이다.
- 그러나 각 연구는 **데이터 정합성 검증 → 전처리·QC → 코호트/라벨 정의 → 모델 선택/학습/검증 → 설명가능성 → 결과 해석 및 논문화**라는 반복 작업을 요구하며, 현재는 이 과정을 **소수의 의료 연구자/AI 엔지니어에게 과도하게 의존**한다.
- 결과적으로 **리드타임 증가, 병목(인력·GPU·도메인 지식), 연구 품질 편차, 외부검증 실패 반복**이 발생하며, “연구 backlog”가 “제품 backlog”로 전환되지 못한다.

### P2. EEG/PSG 연구의 구조적 난제로 인해 ‘반복 가능한 성공’이 어렵다

- EEG/PSG는 본질적으로 **측정 환경·장비·측정자·montage 차이**에 따라 표준화가 어렵고, 신호는 **노이즈/아티팩트가 구조적으로 많다.**
- “클린 EEG의 정의(ground truth)”가 모호하여, 전처리가 과하면 정보 손실, 약하면 노이즈 학습으로 이어지기 쉽다.
- 특히 연구 설계/학습 분할을 잘못하면 **subject-specific signature**를 학습하여 내부 성능은 높고, **external validation에서 급락**하는 패턴이 반복된다.
- 이로 인해 단일 연구를 ‘한 번 성공’시키는 것보다, **성공을 재현 가능한 공정(process)로 만드는 것**이 핵심 문제가 된다. (External validation에 성공한 논문 비율이 높지 않으며, 이에 대한 rigorous investigation이 충분히 이루어지지 않았다.)

### P3. 연구 개발과 제품 개발의 괴리(Research-to-Product Gap)

- 현재는 “연구마다” 파이프라인/코드/전처리/평가 기준이 달라 **재현성·감사 가능성·확장성**이 떨어지고, 연구 산출물이 제품 개발로 이관될 때 **재구현 비용이 재발**한다.
- 즉, 연구 효율화를 위한 자동화가 곧바로 제품 역량이 되지 못하고, 장기적으로는 회사의 “AI 연구 생산성”과 “제품화 속도”를 동시에 제한한다.

### P4. 모달리티 확장 시 기술부채 폭증 위험

- 중장기적으로는 EEG/PSG 단독이 아니라 **text/MRI/PET 등 멀티모달 통합**이 필요하나, 현재 방식대로 모달리티가 추가되면 매번 **처음부터 파이프라인을 재구성**해야 하며 확장 비용이 기하급수적으로 증가한다.
- 따라서 “EEG/PSG 연구 자동화”는 단일 과제의 효율화가 아니라, **향후 멀티모달 연구/제품을 지탱하는 플랫폼 기반**으로 봐야 한다.

## Objectives

- EEG/PSG 데이터셋과 이를 통해 풀고자 하는 research question을 input으로 받으면 research output (최적의 모델, 성능 지표, 그리고 논문) 도출해주는 end-to-end AI co-scientist 개발
- 개발 관련 성과를 정의 가능한 운영 지표로 관리/모니터링하고 원인 분석하여 고도화할 수 있는 사이클 확립
- 본 프로젝트를 통해 개발한 AI co-scientist를 향후 제품 개발에 적용할 수 있도록 초기 개발 단계에서부터 제품화 고려 (임상 - AI doctor / 제약 - clinical trial 실무자를 위한 co-scientist + BD&L 실무자를 위한 DD co-scientist)

## Constraints

### C1. EEG/PSG 데이터의 본질적 제약

- **표준화 어려움(환경/장비/측정자 편차)**
    - 요구사항: 채널/몽타주 매핑, 샘플링/필터 조건 기록, site/device 메타데이터 수집, harmonization 옵션을 시스템 표준으로 제공
- **의미 있는 전처리(denoising) 난이도**
    - 요구사항: “한 가지 전처리 정답”이 아니라 전처리 레시피 버전 관리 + 민감도 분석(sensitivity analysis)가 기본 제공되어야 함
- **분석의 함정: subject-specific leakage 및 외부검증 성능 급락**
    - 요구사항: 기본 분할을 “epoch”가 아니라 **patient-level / site-level / time-split**으로 강제하고, leakage/shortcut 위험을 자동 경고
- **설명가능성의 어려움(noisy/irregular)**
    - 요구사항: xAI는 일반적인 SHAP/Grad-CAM 수준을 넘어, **EEG/PSG 특화(time–channel–frequency) 설명 템플릿**과 “실패 케이스 기반 설명”을 표준 산출물로 제공

### C2. AI 기술의 본질적 제약(특히 LLM/자동화 영역)

- Reverse engineering(=논문 재현), 통계 해석, 결과 해석, 논문 초안 등은 LLM이 **환각/근거 부족/논리 비약** 리스크가 있다.
- 요구사항(가드레일):
    - 통계/평가 지표는 **LLM이 계산하지 않고**, 코드 기반으로 산출
    - 모든 결론은 근거 링크(실험 로그/표/그림/설정)와 함께 제시
    - 중요 산출물(예: SAP/논문 결과 해석)은 **Human-in-the-loop 승인 단계** 포함

### C3. 제품화 방향성 제약(전략 선택 필요)

- 두 가지 방향성이 공존:
    1. **연구별 최적 모델을 빠르게 만들어 ‘연구를 잘하는 Co‑Scientist’**
    2. **EEG/PSG 전반에 통하는 backbone(예: foundation model) + 최소 fine‑tuning으로 재사용 가능한 ‘모달리티 최적화 Co‑Scientist’** 
- 이 제약은 “선택”이 아니라, 단계적 아키텍처(공통 backbone + 연구별 head/adapter)로 풀어야 장기적으로 연구/제품 모두 성립 가능(자세한 의견은 3번에서 제시).

## Personas

### Primary Persona 1: Inhouse Medical Researcher

- **Goals / KPI**
    - 양질의 IRB 연구 수행, 임상적으로 의미 있는 결론 도출, 논문 출판/학회 발표, 협력기관 데이터 확보
- **Jobs-to-be-done**
    - 연구 질문을 “측정 가능한 endpoint/라벨”로 정의하고, 결과를 임상적 문맥에서 해석해 publication-ready 산출물로 만드는 것
- **Pain Points**
    - 분석 파이프라인이 사람/연구마다 달라 **재현성/신뢰성**이 흔들림
    - external validation 실패 시 원인 규명이 어렵고(데이터 문제인지 모델 문제인지), 재시도 비용이 큼
    - 논문화(Methods/Results/Limitations) 작성이 반복 노동으로 누적
- **Needs from Co‑Scientist**
    - 연구 질문→Task spec/endpoint 제안, 코호트/라벨 설계 지원
    - 결과에 대한 **근거 기반 해석 + 한계/추가 분석 제안**
    - 논문/IRB 보고서 초안 생성(근거 연결 포함)
- **Success Criteria**
    - 동일한 연구를 “다른 사람이/다른 시점에” 실행해도 동일 결론에 도달
    - 결과의 임상적 타당성(설명가능성 + subgroup 분석 + bias 점검)이 확보됨

### Primary Persona 2: Inhouse AI Engineer (모델 개발자)

- **Goals / KPI**
    - 연구 질문을 성능/재현성/외부검증을 만족하는 딥러닝 모델로 구현, 리드타임 단축
- **Jobs-to-be-done**
    - 데이터 불확실성(노이즈/라벨/표준화)을 관리하면서, baseline~SOTA까지 비교 가능한 실험을 구축
- **Pain Points**
    - 전처리/QC/코호트/평가가 매 연구마다 새로 짜여 **기술부채**가 누적
    - 환자 단위 분할/외부검증/도메인 쉬프트 대응이 “사람 기억”에 의존
    - xAI/리포트/논문화까지 엔지니어가 떠안아야 하는 범위가 과도
- **Needs from Co‑Scientist**
    - 실험 템플릿/모델 레지스트리/평가 표준화
    - leakage 방지/외부검증 프로토콜 자동화
    - xAI·에러분석·리포트 자동 생성으로 “마지막 30%” 비용 절감
- **Success Criteria**
    - 같은 데이터/설정에서 실험 결과가 재현되고, 연구 간 비교가 가능(leaderboard)

### Secondary Persona 3: Inhouse Non‑AI Engineer (데이터/플랫폼/MLOps)

- **Goals / KPI**
    - 데이터 파이프라인 안정성, 보안/접근통제, 실험 인프라 운영, 배치/스케줄링
- **Pain Points**
    - EDF/임상DB/파일 서버/annotation이 분절되어 데이터 라인리지 관리가 어려움
    - IRB 조건 및 PHI 통제가 코드/사람에 의존
- **Needs**
    - 데이터 카탈로그, RBAC, 감사로그, job orchestration, 환경 고정(컨테이너) 등 “연구 운영체제” 기능
    - EDF 변환 프로세스

### Expansion Personas (제품화)

- **Clinician (임상현장)**
    - KPI: 빠르고 정확한 진단/처방, 업무 부담 감소
- **Pharma R&D / Clinical Scientist**
    - KPI: 효율적 임상시험 설계(엔드포인트/바이오마커/환자 층화), 데이터 해석 및 의사결정(Go/No-Go)
    - Needs: 고정 분석 파이프라인, 품질 모니터링, 중간분석/최종 리포트 자동화
- **Pharma BD&L (DD)**
    - KPI: 후보물질 평가/인라이선싱 의사결정
    - Needs: (확장 시) 논문/임상 데이터/바이오마커 근거를 구조화해 DD 관점으로 요약·비교

## User Scenarios

### Scenario 1) Inhouse medical researcher: 연구 질문 → 1차 결과/리포트까지

1. Medical researcher가 연구 질문을 자연어로 입력(예: 특정 질환군에서 약물 A의 반응 예측, 혹은 예후 event 예측).
2. Co-Scientist가 질문을 Task spec으로 변환: target/Responder 정의 후보, time window(베이스라인/추적), 평가 지표, 잠재적 confounder를 제안.
3. 데이터 업로드/연결 후 자동 QC와 표본수/결측/라벨 분포를 요약하고, 연구 수행 가능성 및 리스크(불충분한 표본, 편향 가능성)를 리포트.
4. 안전한 분할 전략(patient-level, site-level, time-split)을 기본값으로 설정하고 leakage 위험을 사전 차단.
5. 베이스라인 모델과 후보 모델을 학습·검증하고 결과 성능표 + calibration/subgroup 분석을 생성.
6. xAI 및 실패 케이스 분석을 포함한 “연구자용 해석 리포트”와 논문 초안을 생성(Methods/Results/Limitations 포함).
7. Researcher는 리포트를 기반으로 추가 분석(서브코호트/민감도 분석)을 요청하거나, IRB/학회/논문화로 진행.

### Scenario 2) Inhouse AI Engineer: 신규 연구 투입 시 ‘재현 가능한 실험 공정’으로 표준화

1. AI 엔지니어가 신규 데이터셋/태스크를 등록하면, Co-Scientist가 데이터 카드(Data Card) 및 실험 템플릿을 자동 생성.
2. 전처리 레시피(필터링/리레퍼런싱/epoching)와 QC 게이트가 기본 세팅되고, 버전 관리(파라미터/환경)가 자동 기록.
3. 모델 전략(공개/재현/신규) 후보를 제시하고, 최소 2~3개 베이스라인을 자동 실행해 리더보드로 비교.
4. 외부검증 성능이 급락하면, 원인 분해 리포트(품질/라벨/분포/도메인 쉬프트/누수 가능성)를 제공하고 다음 액션을 추천.
5. 결과 및 설정은 Run ID로 저장되어 재실행/재현이 가능하며, 이후 다른 연구와 비교 가능한 형태로 축적된다.

### Scenario 3) 논문 기반 재현(Replication): 문헌 결과를 내부 데이터에 이식해 베이스라인 확보

1. 연구자가 특정 논문을 선택하고(코드/방법론 포함), Co-Scientist가 해당 방법을 기반으로 재현 실험 템플릿을 생성.
2. 논문 조건(전처리/분할/지표)을 가능한 범위에서 재현하고, 내부 데이터에서의 성능과 차이를 비교 분석.
3. 차이가 발생하면 데이터 특성/코호트 차이/전처리 차이를 설명하고, 내부 데이터에 최적화된 수정안을 제안.
4. 결과적으로 “논문 대비 내부 재현성”과 “내부 최적화 모델”이 모두 문서화되어, 향후 제품화/대외 커뮤니케이션 근거로 축적된다.

### Scenario 4) 제약사 Trial 확장(Feasibility Pack): EEG/PSG 기반 엔드포인트 후보 검증

1. Clinical Scientist가 trial 컨셉(질환/기전/약물/예상 효과)을 입력하고, 후보 엔드포인트(EEG/PSG biomarker)를 정의.
2. Co-Scientist가 기존 데이터로 feasibility(변별력, 변동성, 결측/품질 민감도, 표본수 가정)를 빠르게 평가.
3. “고정 분석 파이프라인(버전)”과 검증 리포트를 생성해 trial 프로토콜/SAP 초안에 포함 가능한 형태로 제공.
4. 결과적으로 엔드포인트 채택/측정 스케줄/품질 기준이 초기부터 표준화되어 시행착오를 줄인다.

### Scenario 5) 제약사 Trial 운영(Operations Monitor): 데이터 품질·드리프트·중간분석 지원

1. 사이트별 EEG/PSG 품질 지표를 모니터링하고, 결측/노이즈/장비 편차 증가를 조기 경보.
2. 중간분석 시점에서 데이터 잠금(DBL) 전 QC 리포트와 모델 기반 탐색 분석을 생성.
3. 최종적으로 외부감사/규제 대응을 고려한 산출물(라인리지/로그/설정 고정)을 패키징해 제공한다.

### Scenario 6) 기존 연구 기반 제약사 Trial 고도화

---

# PRD

## Product Scope

### In Scope

- **Input**
    1. EEG/PSG raw + annotation/event(수면단계, 호흡 이벤트 등) + (가능하다면) text data (EEG 판독문, EHR raw data 등)
    2. Pair된 임상 데이터(진단/예후/약물 반응/인구통계/검사/동반질환/처방/추적) 
    3. 연구 질문(자연어 + 구조화 입력 옵션) 
- **Core Output (연구 산출물 패키지)**
    1. **Task spec & 분석계획(초안)**: endpoint/라벨 정의, split 전략, confounder 체크, 평가 지표 정의
    2. **모델/파이프라인 결과**: 베이스라인~후보 모델 성능표, 외부검증 결과, calibration/subgroup
    3. **xAI & error analysis**: time–channel–frequency 설명 + 실패 케이스 요약
    4. **해석 리포트 + 논문/IRB 초안**: Introduction/Methods/Results/Discussion/Limitations + 다음 실험 제안
        1. Introduction에서는 최신 연구 동향을 적절히 반영하였는지, 양질의 논문들을 균형있게 사실과 어긋나지 않게 활용하였는지, 논리가 자연스러운지 등을 평가해야 함
        2. Discussion에서는 xAI 및 failure case 분석에 관한 내용이 들어가야 함
- **운영**
    - 실험 추적(Experiment tracking), 데이터 라인리지, 접근권한, 감사로그

### Out of Scope

- 임상 의사결정 지원(진료용)으로의 직접 배포(규제/검증 트랙 별도)
- 멀티모달(MRI/PET) end-to-end 학습의 본격 지원(아키텍처 확장성만 선반영)
- 비공개 상용모델의 역공학/가중치 복제(정책상 제외; 논문 기반 재현만 지원)

## Product Layer

### Layer 0: Governance & Reproducibility Plane (교차 레이어)

- 데이터/실험/산출물에 대한 **라인리지, RBAC, 감사로그**
- 컨테이너 기반 실행 환경 고정, seed/버전 고정x
- “검증 게이트”: 외부검증/누수 체크/결측률/품질 기준을 통과하지 못하면 결과 확정 불가

### Layer 1: Research Orchestrator (의사결정/설계)

- 자연어 연구 질문 → **구조화 Task spec(JSON 스키마)**
    - task type(분류/회귀/생존/검출/세그), target 정의, time window, split 전략, confounder 후보, 평가 지표
- 연구 설계 리스크 자동 탐지:
    - leakage(시간 역전/환자 중복), label leakage, site bias, class imbalance, small‑n 경고
- 모델 전략 선택(공개 모델 / 논문 재현 / 신규 모델) + 선택 근거(데이터 크기/레이블/외부검증 요구)
- 실험 결과를 “판정”하고 다음 액션 추천(추가 코호트/정의 수정/민감도 분석)

### Layer 2: ML Factory (전처리~학습~평가 자동화)

- EEG/PSG 전처리 템플릿(필터/리레퍼런싱/아티팩트/epoching) + 레시피 버전관리
- 임상 데이터 ETL/정제/코호트/라벨 정렬 + CDM/스키마 매핑(내부 표준)
- 학습/검증/테스트 분할을 “기본값으로 안전하게”: patient/site/time split 지원
- 튜닝/교차검증/모델 비교 리더보드 + 통계적 불확실성(CI/부트스트랩)

### Layer 3: Evidence & Explanation (xAI + LLM 해석)

- xAI: EEG/PSG의 특성에 맞춘 **설명 템플릿 표준화**(time–channel–frequency, case-based)
- LLM: “산출물 서술” 중심 + 근거 링크 + 승인 흐름
    - 통계/평가 수치는 코드 산출물만 참조(LLM이 계산 금지)
    - CoT는 내부 로그로만 보관, 사용자에게는 근거 기반 요약 제공

## Features In

### 1. Data Ingestion & Governance

- EEG/PSG:
    - 포맷 지원: EDF/EDF+ 등
    - 채널 매핑/명명 규칙 정규화(기관/장비별 alias)
    - 이벤트/annotation import(수면단계, apnea/hypopnea, arousal 등)
- 임상 데이터:
    - 환자 키 매칭(가명화된 join 키), 시간축 정렬(visit/date)
    - 진단/예후/약물반응/인구통계/처방/검사 결과 스키마 표준화
- 거버넌스:
    - 접근제어(RBAC), 감사로그, 데이터 라인리지(어떤 데이터로 어떤 모델이 생성되었는지)
    - IRB 조건(목적/기간/반출 제한) 메타데이터로 강제

### 2. Preprocessing & QC (EEG/PSG 특화)

- 자동 품질지표:
    - 신호 dropout, 과도한 노이즈, 채널 임피던스 이슈 추정, 라벨 불일치
- 아티팩트 처리:
    - 기본 규칙 기반 + ML 기반(옵션) → 어떤 종류의 아티팩트에 대해서 어느 수준으로 쳐낼지에 대한 선행연구 기반 의학적 고민 필요
- Epoching / time‑frequency 변환 옵션:
    - task별 템플릿(수면단계/이벤트 검출/약물반응 예측 등)
- “전처리 레시피” 버전 관리:
    - 파라미터/코드/환경 고정(재현성)

### 3. Cohort Builder & Labeling Studio (임상 연구 중심)

- 포함/제외 기준을 UI로 구성(예: 나이, 진단코드, 약물 투여 이력, PSG 품질 조건)
- 라벨 정의 도우미:
    - prognosis(예: 특정 기간 내 event), drug response(Responder 정의), diagnosis 분류 등
- Confounder/Leakage 체크:
    - 예: outcome 이후 측정된 feature 사용 경고
    - 환자 단위 분할 강제(동일 환자 데이터가 train/test 섞이는 문제 방지)

### 4. Model Strategy Engine (가져오기/재현/신규)

1. 공개 모델 가져오기
    - 모델 레지스트리(“모델 동물원”) + 라이선스/학습데이터/적용가능 task 메타데이터
    - zero‑shot/finetune 옵션
2. 논문 재현(“reverse engineering”의 제품화 버전)
    - 권장 표현/정책: **논문 기반 재구현/재현(Replication)**
    - 입력: 논문/코드/설명 → 파이프라인 템플릿 생성 → 재학습/재현 성능 리포트
3. 신규 모델 생성
    - 아키텍처 템플릿:
        - EEG/PSG 단일모달: CNN/Transformer/TCN/Time‑freq 기반
        - 멀티모달: (EEG/PSG encoder) + (Tabular clinical encoder) fusion
        - 멀티태스크: 진단+예후+약물반응 공동학습(가능할 때)
    - 자기지도학습(옵션): 대규모 unlabeled PSG/EEG로 representation 선학습 후 다운스트림
4. **공통 Backbone(Foundation/Pretrained) + 연구별 Adapter/Head**
    - “전반적 EEG/PSG 최적화” 방향성을 제품에 내재화(3번 의사결정과 연결)

### 5. Training/Evaluation Factory

- 실험 관리:
    - 데이터 버전, 코드 버전, 파라미터, seed, 환경(컨테이너) 고정
- 평가:
    - 분류: AUROC/AUPRC, calibration, subgroup 성능
    - 예후: c‑index, time‑dependent AUC
    - 약물반응: uplift/ITE(필요 시), 또는 responder 분류
- 통계/검정:
    - 부트스트랩 CI, 교차검증, 비교(모델간 유의성)
    - 통계분석 결과
- 배포는 “연구용”이므로:
    - 임상 의사결정 직접 사용은 제한(“연구용” 표기), export 시 고지 문구 자동 포함

### 6. xAI/Interpretability Suite (모델 최적화된 xAI)

- EEG/PSG 특화:
    - 시간 구간 중요도(어떤 epoch가 결정에 기여?)
    - 채널 중요도(어떤 lead?)
    - 주파수 밴드 기여(가능 시)
- Tabular 임상 특화:
    - 변수 중요도, 상호작용, counterfactual(“이 변수 범위를 바꾸면 예측이 어떻게 변하는가”)
- 결과물 표준:
    - “해석 가능 산출물”을 보고서 템플릿으로 고정(연구마다 형식 들쭉날쭉 방지)

### 7. LLM Layer (Co‑Scientist Copilot)

- 입력 컨텍스트:
    - 연구 질문, 코호트/라벨 정의, 실험 로그, 결과 지표, xAI 결과
    - 내부 문서(RAG): SOP, 분석 표준, 용어집, 과거 연구 리포트, (허용 시) 관련 논문
- 출력:
    - 결과 요약(의학적 의미 중심)
    - 한계/위험(데이터 편향, 일반화, confounding 가능성)
    - 다음 실험 제안(추가 ablation, sensitivity analysis, subgroup)
    - IRB/내부 보고/논문 섹션 초안(Methods/Results/Limitations)
- “CoT” 운영 원칙(제품 관점):
    - 사용자에게는 **근거 기반 요약**을 제공하고,
    - 내부적으로는 **결정 로그/근거 링크/설정**을 저장해 감사 가능성을 확보

## Features Out

TBD

## Success Metrics

### A0) 의학적 “Critical Fail” (하나라도 발생하면 해당 연구 Run은 실패)

- (CF-M1) **Label/endpoint 정의가 결과 해석과 불일치**(예: responder 기준이 문서와 코드가 다름)
- (CF-M2) **환자 단위 분할 위반 또는 시간역전(leakage)로 결론이 왜곡될 위험이 높은데도 경고/차단이 없었음**
- (CF-M3) **외부검증이 필요한 상황(기관/장비/기간이 다른 데이터 존재)에서 외부검증 없이 “일반화 결론”을 서술**
- (CF-M4) RUO 범위를 넘는 **진료 권고/치료 효과 확정/인과 주장**을 결과 보고서가 포함

---

### A1) Endpoint/Label 임상 타당성 검토 통과율

- **정의:** 연구별로 “타깃(진단/예후/약물반응) 정의”, “시간 창”, “포함/제외 기준”, “주요 혼란변수 후보”가 임상적으로 타당하고 재현 가능한지
- **측정:** 임상의 1인 + (가능하면) 통계/역학 1인의 체크리스트 리뷰
    - 체크리스트 항목 예: 정의의 명확성, 시간축 적합성, 임상적 의미, confounding 위험 인지 여부, 재현 가능성
- **통과 기준:** 필수 항목(예: 10개) 중 **중대 결함 0개**, 경미 결함은 수정 후 통과 가능
- **과대평가 방지:** LLM이 “타당하다”라고 서술해도 인정하지 않으며, **리뷰어 체크리스트 결과만** 지표로 반영

---

### A2) 외부검증(Generalization) 성능 유지율

- **정의:** 내부검증 대비 외부검증에서 성능이 과도하게 붕괴하지 않는지(EEG/PSG의 핵심 리스크 대응)
- **측정:** 최소 1개의 external split(기관/장비/기간 분리 중 하나 이상)에서 평가
- **통과 기준(예시 템플릿):**
    - 분류: 외부 AUROC가 내부 AUROC 대비 **Δ ≤ 0.10** 이내 또는 내부 대비 **80% 이상 유지**
    - 예후/생존: 외부 c-index가 내부 대비 **Δ ≤ 0.08** 이내
    - (데이터 규모가 작아 변동이 큰 경우) 부트스트랩 CI로 “급락 여부”를 판단(급락이 CI로 설명되지 않으면 실패)
- **과대평가 방지:** 외부검증이 없으면 “성공” 판정 불가(지표는 N/A가 아니라 Fail)

---

### A3) Calibration(예측 확률의 신뢰도) 적합성

- **정의:** 예측 확률이 실제 발생률과 정합적인지(임상 연구/시험에서 중요)
- **측정:** ECE(Expected Calibration Error), Brier score, calibration slope/intercept
- **통과 기준(예시):**
    - ECE ≤ 0.05 또는 baseline 대비 개선
    - calibration slope가 0.8~1.2 범위(데이터 크기 고려하여 CI 포함)
- **과대평가 방지:** 성능(AUROC)만 좋고 calibration이 나쁘면 “의학적 성공”으로 인정하지 않음(별도 실패 처리 가능)

---

### A4) Subgroup Robustness / Bias 점검 완결성

- **정의:** 인구통계(성별/연령대), 기저질환, 장비/사이트 등 주요 하위집단에서 성능이 일관적인지
- **측정:** 사전에 정의된 subgroup 리스트에 대해 성능/캘리브레이션을 보고
- **통과 기준(예시):**
    - 주요 subgroup 간 AUROC 격차가 0.10 초과 시 원인 분석/완화 시도 및 보고서에 명시(미수행 시 Fail)
    - 최소한 “성별/연령/사이트(or 장비)” 3축은 기본 포함
- **과대평가 방지:** “데이터가 작아서 못했다”는 문장만으로 면책 불가.
    - *대안 제시(예: 병합, bootstrap, 추후 데이터 수집 계획)**까지 포함해야 Pass

---

### A5) xAI 임상 합리성(Plausibility) 점수

- **정의:** xAI가 제시한 시간/채널/주파수 기여가 “임상적/생리학적으로 해석 가능한 형태”인지
- **측정:** 임상의 블라인드 리뷰(모델 성능을 숨긴 상태에서 xAI만 보고 평가)
- **루브릭(예시 0~2점, 항목 5개):**
    - (1) 설명이 특정 노이즈/아티팩트에 과도하게 반응하는가
    - (2) 알려진 생리/수면 구조와 모순되는가
    - (3) 설명이 환자 단위/세션 단위로 일관적인가
    - (4) 대표 케이스/실패 케이스가 충분히 제공되는가
    - (5) 설명이 “행동 가능한 추가 분석”으로 연결되는가
- **통과 기준:** 총점 **7/10 이상**, 그리고 (1) 항목에서 “심각한 아티팩트 의존” 판정이면 Fail
- **과대평가 방지:** LLM의 “해석 문장”이 아니라 **xAI 산출물 + 리뷰 점수**만 반영

---

### A6) 결론의 과잉 주장(Overclaiming) 제로 지표

- **정의:** 연구 결과 보고서/논문 초안이 근거 이상의 임상적 확정(효과, 인과, 진료 권고)을 하지 않는지
- **측정:** 자동 룰 기반 탐지 + 샘플링 인적 검수
    - 예: “치료 효과가 입증”, “반드시”, “진단 가능”, “처방 추천” 등 금지 표현/패턴
- **통과 기준:** **중대 위반 0건**(발견 시 해당 Run Fail)
- **과대평가 방지:** LLM이 자신감 높은 문장을 쓰지 못하도록 “금지 패턴”을 시스템적으로 차단/교정

---

### A7) 연구 산출물의 ‘재현 가능한 결론’ 비율

- **정의:** 동일 데이터/동일 설정 재실행 시 임상 결론이 바뀌지 않는지
- **측정:** 동일 Run을 seed만 바꿔 N회(예: 3회) 반복해 주요 결론(방향성/유의한 비교)이 유지되는지 확인
- **통과 기준(예시):** 핵심 결론(Top-line) 일치율 **≥ 2/3**, 성능 지표 변동이 사전 정의된 허용 범위 내
- **과대평가 방지:** 한 번의 좋은 결과만으로 성공 판정 금지(최소 반복)

---

## B. 공학적 Success Metrics (Engineering / System / Agent Reliability)

### B0) 공학적 “Critical Fail” (하나라도 발생하면 해당 연구 Run은 실패)

- (CF-E1) 데이터 라인리지/버전/환경이 누락되어 결과 재현이 불가능
- (CF-E2) 보고서 내 수치(예: AUROC, 표본수, p-value)가 실제 산출물과 불일치(숫자 오류/환각)
- (CF-E3) PHI/개인정보가 보고서/로그/Export에 포함되거나 반출 정책 위반
- (CF-E4) 시스템이 사용자 승인 없이 데이터 결합/반출/대규모 비용 작업을 실행

---

### B1) End-to-End 파이프라인 성공률(무중단 실행)

- **정의:** 입력(데이터+질문)부터 결과 리포트 생성까지 “중요 오류 없이” 완료되는 비율
- **측정:** 월/분기 기준 전체 Run 대비 성공 Run 비율
- **통과 기준(예시):** v1에서 **≥ 85%** (잔여 15%는 데이터 품질/포맷 이슈 등으로 분류 및 개선 backlog로 반영)
- **과대평가 방지:** “중간에 사람이 고쳐서 겨우 완료”는 성공으로 집계하지 않고, **자동 실행 기준**으로 분리 집계

---

### B2) 재현성(Reproducibility) 지표

- **정의:** 동일 Run ID를 재실행했을 때 결과가 허용오차 내에서 동일한지
- **측정:** 동일 코드/환경/seed로 재실행 시 결과 해시(또는 주요 지표) 일치 여부
- **통과 기준:**
    - 동일 seed: 주요 지표(성능/표본수/분할)가 **100% 일치**
    - seed 변경: 변동 폭이 허용 범위 내(사전 정의)
- **과대평가 방지:** LLM 서술로 “재현된다” 주장 불가. **실행 로그 기반**으로만 판정

---

### B3) 데이터 라인리지/감사로그 완전성

- **정의:** 어떤 데이터(버전)로 어떤 전처리/모델/평가가 수행되었는지 추적 가능해야 함
- **측정:** 필수 메타데이터 필드(예: 데이터 버전, 환자 수, split 키, 전처리 레시피 버전, 모델/하이퍼파라미터, 환경 컨테이너 해시)가 모두 채워졌는지
- **통과 기준:** 필수 필드 누락률 **0%** (누락 시 Run Fail)
- **과대평가 방지:** 일부 필드라도 누락되면 “보고서는 생성되었어도” 성공 인정하지 않음

---

### B4) LLM 보고서 수치 정합성(Numeric Consistency) 점수

- **정의:** LLM이 보고서에 기재한 숫자가 실제 산출물과 일치하는지(환각 방지 핵심)
- **측정:**
    - (1) 보고서에서 숫자/지표를 파싱
    - (2) 실험 산출물(표/JSON/로그)의 해당 값과 자동 비교
- **통과 기준:**
    - 주요 지표(표본수, AUROC/AUPRC, CI, subgroup 성능 등) **불일치 0건**
    - 경미 지표는 허용오차(예: 소수점 반올림 ±0.01) 내
- **과대평가 방지:** 불일치 1건이라도 있으면 **해당 보고서는 “승인 불가”**로 처리(자동 수정 시도는 가능하되, 수정 후 재검증 통과가 필요)

---

### B5) 근거 연결(Evidence Grounding) 커버리지

- **정의:** 보고서의 핵심 주장/결론이 실험 산출물(표/그림/로그/xAI 결과)에 “링크”되어 있는지
- **측정:** “핵심 주장”을 규정(예: Top 10 claims)하고, 각 claim이 최소 1개 이상의 artifact 참조를 갖는지 자동 체크
- **통과 기준:** 핵심 주장 근거 연결률 **≥ 95%**, “Top-line 결론”은 100% 요구
- **과대평가 방지:** LLM의 표현이 그럴듯해도 근거 링크가 없으면 Fail 처리

---

### B6) 보고서 품질 루브릭(정성 지표의 정량화)

- **정의:** 보고서가 연구 문서로서 최소 요구사항(Methods/Results/Limitations/Next steps)을 충족하는지
- **측정:** 0~2점 루브릭(예: 총 10항목, 20점 만점) + 블라인드 샘플링 리뷰
    - 예: (1) Methods에 split/전처리/지표가 명시, (2) 한계에 leakage/편향/일반화가 언급, (3) 다음 실험 제안이 구체적…
- **통과 기준:** **14/20 이상**, 그리고 필수 항목(Methods의 분할/지표 명시)은 미충족 시 Fail
- **과대평가 방지:** “잘 썼다” 류의 주관 평가가 아니라, **항목 충족 여부**로만 점수화

---

### B7) 시간/비용 효율(Time-to-First-Result, Cost per Study)

- **정의:** 연구자가 최초로 의사결정 가능한 결과(베이스라인 성능표+QC+기본 해석)를 얻기까지 걸리는 시간/비용
- **측정:**
    - TFR: 데이터 연결 완료 시점 → 1차 리포트 생성 시점
    - GPU 시간/스토리지/실행 횟수 기반 비용(내부 산식)
- **통과 기준(예시):**
    - TFR: 기존 대비 **50% 이상 단축**(또는 목표 절대시간 X시간 이내)
    - 연구당 GPU 시간: baseline 대비 **동일 또는 감소**, 성능 개선이 있을 경우 증가 허용
- **과대평가 방지:** 사람이 수동 개입한 시간은 TFR에서 제외하지 않고 포함(즉, 자동화가 실제로 시간을 줄였는지 확인)

---

### B8) QC 탐지 성능(데이터 품질 이슈의 자동 발견)

- **정의:** dropout/노이즈/라벨 불일치 등 “주요 품질 문제”를 놓치지 않는 능력
- **측정:** 과거 이슈 케이스(known bad cases) 또는 라벨된 QC 세트로 재현 테스트
- **통과 기준(예시):**
    - 중대 품질 이슈 탐지 recall **≥ 0.90**
    - false negative(놓침) 발생 시 원인/대응이 Run 리포트에 자동 기록
- **과대평가 방지:** precision이 다소 낮아도(과경고) 허용 가능하나, **놓침(FN)**은 임계적으로 취급

---

### B9) 모델 전략 선택의 후회(regret) 최소화

- **정의:** Orchestrator가 선택한 전략(공개/재현/신규)이 “근거 없이 비효율”을 만들지 않는지
- **측정:** 동일 데이터에서 최소 2개 대안 전략을 제한적으로 비교하여 성능/비용을 평가
- **통과 기준(예시):**
    - 성능 regret(최고 성능 모델 − 선택 모델)이 평균 **≤ 0.03(AUROC 기준)**
    - 비용 regret(GPU 시간 등)이 평균 **≤ 20%**
- **과대평가 방지:** LLM이 “이게 최선”이라고 말해도 인정하지 않으며, **실험 비교 결과로만** 판정

---

### B10) 보안/반출 통제 준수율

- **정의:** RBAC, 감사로그, export 정책, PHI 마스킹이 실제로 강제되는지
- **측정:** 보안 테스트(권한 없는 접근 시도, 금지 반출 시도) + 감사로그 점검
- **통과 기준:** 위반 **0건**(발생 시 즉시 Fail 및 핫픽스 대상)

## Open Issues

TBD

## Q&A

TBD

---

# 개발 Phase

## Phase 0 — 골든패스 정의 + Artifact 계약/검증 규칙 고정

### 포함 기능(Features In 매핑)

- Feature 자체 구현이 아니라, **후속 Phase의 성공 조건을 정의**하는 단계
    
    (Layer0~3 전체의 인터페이스/계약을 고정하는 것이 핵심)
    

### 핵심 산출물(Artifacts / DoD)

- 골든패스 연구 2개 확정(예: PSG 기반 진단/스테이징 1개 + EEG/PSG+임상 기반 예후/약물반응 1개)
- 스키마(초안이 아니라 “검증 가능한 계약”):
    - `DatasetManifest`, `RunManifest`
    - `TaskSpec`, `CohortSpec`, `SplitPlan`, `EvaluationPlan`
    - `PreprocessRecipe`, `ModelRecipe`
    - `MetricsBundle`, `ExplainBundle`, `ReportBundle`
- Validator 룰(필수):
    - 환자 단위 분할 위반/시간역전/중복 leakage 탐지 규칙
    - 보고서 숫자 정합성 검사 규칙(나중에 LLM 붙일 때 강제)
        - 금지 주장(진료 권고/인과 확정) 탐지 규칙

### KPI(공학)

- (E0-1) **스키마 필수 필드 정의 완료율 100%** (미정 필드 0)
- (E0-2) **Validator 규칙 커버리지**: 최소 10개 핵심 규칙(누수/정합성/금지문구/근거링크)

### KPI(의학)

- (M0-1) 골든패스 2개에 대해 endpoint/라벨 정의 체크리스트 **중대 결함 0개**(임상의 리뷰)
- (M0-2) “외부검증 필요 조건”을 EvaluationPlan에 **명시**(가능하면)

### 권장 기간

- **1~2주**

---

## Phase 1 — Layer 0 기반(Study/Run/Artifact Backbone) 구축

### 포함 기능(Features In 매핑)

- **Feature 1(Data Governance) 중 거버넌스/라인리지/저장**
- (후속을 위해 필수) 실행 오케스트레이션/GPU 큐의 최소 기능

### 개발 범위(구현 항목)

- Study Workspace 개념(연구 단위 엔터티)
- Dataset 등록/버전 관리(Manifest 저장)
- Run 생성/실행/재실행(Manifest 저장)
- Artifact Store(모델/지표/그림/리포트 저장)
- RBAC 최소(역할 2~3개) + 감사로그 최소(누가/언제/무엇)

### 핵심 산출물(DoD)

- 동일 입력+동일 설정으로 **Run 재실행 시 결과 재현**
- 모든 결과물이 Run ID로 추적되고 artifact 링크로 연결됨

### KPI(공학)

- (E1-1) **라인리지 필수 필드 누락률 0%** (DatasetManifest/RunManifest)
- (E1-2) 동일 Run 재실행(동일 seed) 시 **핵심 지표 해시 100% 일치**
- (E1-3) End-to-End 파이프라인(더미) 실행 성공률 **≥ 90%** (데이터 처리 전 “프레임워크” 기준)

### KPI(의학)

- 이 단계는 의학 지표 산출 전 단계라, **의학 KPI는 N/A**(대신 데이터 반출/PHI 통제 게이트가 의학적 리스크를 줄임)

### 권장 기간

- **2~3주**

---

## Phase 2 — EEG/PSG Ingest + QC + Preprocess Recipe (Feature 1,2의 실체)

### 포함 기능(Features In 매핑)

- **Feature 1:** EEG/PSG ingest(EDF/EDF+), 채널/몽타주 메타데이터 추출/정규화
- **Feature 2:** QC 지표, 전처리 레시피 버전관리, QC gate

### 개발 범위(구현 항목)

- EDF 파서 + annotation import(가능 범위)
- 채널 alias 맵핑(내부 표준으로 매핑)
- QC metrics:
    - dropout 비율, 채널 이상, 과도 노이즈(간단 지표부터)
- PreprocessRecipe:
    - 필터/리레퍼런싱/epoching 최소 템플릿 1~2개
- QC gate:
    - “학습 진행 가능/불가” 판정 + 사유 기록

### 핵심 산출물(DoD)

- `EEG/PSG Data Card` 자동 생성(QC 요약 포함)
- 골든패스 데이터에서 **QC 통과/실패가 재현 가능**하게 분류됨

### KPI(공학)

- (E2-1) EDF ingest 성공률 **≥ 95%** (지원 포맷 범위 내)
- (E2-2) PreprocessRecipe 재실행 시 결과(샘플 수/epoch 수) **100% 재현**
- (E2-3) QC gate 실행 시간: 파일 1개당 **X분 이내**(내부 기준 설정)

### KPI(의학)

- (M2-1) QC 리포트가 임상의/연구자에게 “의미 있는 수준”인지 블라인드 리뷰(루브릭)
    - 예: 10항목 중 **7점 이상**(특히 “명백한 저품질 케이스를 놓치지 않았는가”)
- (M2-2) QC 실패 케이스에 대해 “왜 제외해야 하는지” 근거가 리포트에 명시(설명 가능성)

### 권장 기간

- **3~4주**

---

## Phase 3 — 임상 데이터 Ingest + Cohort/Label/Split 안전장치 (Feature 1,3)

### 포함 기능(Features In 매핑)

- **Feature 1:** 임상 데이터 ingest/정렬(가명 join, time alignment)
- **Feature 3:** cohort builder, label 정의 도우미, leakage/confounding 체크, SplitPlan

### 개발 범위(구현 항목)

- 임상 테이블 최소 스키마(진단/인구통계/약물/추적결과) 매핑
- CohortSpec UI(또는 YAML/JSON 기반으로 시작 가능)
- Label 정의: prognosis / responder(약물반응) 템플릿 1~2개
- SplitPlan:
    - patient-level split 강제
    - (가능하면) site/time split 옵션
- Leakage Validator:
    - 환자 중복/시간역전(Outcome 이후 feature), label leakage 가능성 경고/차단

### 핵심 산출물(DoD)

- `CohortSpec + LabelSpec + SplitPlan`이 확정되면, 동일 데이터에서 **항상 같은 cohort/split**이 생성
- leakage 규칙 위반 시 **Run Fail 또는 Hard Warning(정책 선택)**

### KPI(공학)

- (E3-1) SplitPlan 강제 준수율 **100%** (patient-level 위반 0)
- (E3-2) Leakage Validator 커버리지: 최소 5종(중복/시간역전/이후측정/사이트편향/라벨누수)
- (E3-3) 코호트 생성 시간: **X분 이내**(내부 기준)

### KPI(의학)

- (M3-1) Endpoint/라벨 정의 체크리스트 통과율 **≥ 90%** (중대 결함 0)
- (M3-2) Confounding 위험/한계가 spec에 명시되었는지(필수 항목 누락률 0)
- (M3-3) “외부검증 필요 시나리오”가 명시되고, 외부검증 없이 일반화 서술이 금지됨(정책 포함)

### 권장 기간

- **2~4주**
    
    (임상 데이터 스키마 정리가 되어 있으면 2주, 그렇지 않으면 4주 이상도 흔합니다.)
    

---

## Phase 4 — Baseline Modeling + Evaluation Harness (Feature 4(부분), 5)

### 포함 기능(Features In 매핑)

- **Feature 5:** Training/Evaluation Factory(실험/평가/통계)
- **Feature 4(부분):** “모델 전략 엔진” 전체가 아니라, v1에서는 **baseline 템플릿 + 모델 레지스트리 최소**만

### 개발 범위(구현 항목)

- Baseline 모델 2~3개:
    - (예) 간단 CNN/Transformer 1개 + tabular-only 1개 + fusion 1개(가능하면)
- 학습 파이프라인(컨테이너/seed 고정)
- MetricsBundle 자동 생성:
    - AUROC/AUPRC, calibration(ECE/Brier), subgroup(성별/연령/사이트 최소), CI(부트스트랩)
- External validation 템플릿(가능한 경우)

### 핵심 산출물(DoD)

- 골든패스 2개에서 “버튼 1번”으로 **성능표 + calibration + subgroup + CI**까지 생성
- 결과가 ReportBundle/ExplainBundle의 입력으로 쓸 수 있게 구조화됨

### KPI(공학)

- (E4-1) Time-to-First-Result(TFR): 데이터 연결 후 **1차 성능표 생성까지 X시간 이내**(내부 목표 설정)
- (E4-2) 실험 재현성: 동일 seed 재실행 시 **지표 100% 일치**
- (E4-3) 자동 실행 성공률(사람 개입 없이): **≥ 80%** (v1 초기 현실적 기준)

### KPI(의학)

- (M4-1) 외부검증 성능 유지율(가능 시): 내부 대비 **80% 이상 유지** 또는 Δ 허용범위 내
- (M4-2) Calibration 기준 통과: ECE/Brier가 baseline 대비 **개선 또는 기준 이하**
- (M4-3) Subgroup 성능 보고 완결성: 지정 subgroup 모두 리포트되고, 큰 격차 발생 시 원인/한계가 자동 기술됨(미기술 시 Fail)

### 권장 기간

- **3~5주**

---

## Phase 5 — xAI + Error Analysis + “LLM 없이도 완성되는” 리포트 번들 (Feature 6 + 7(부분))

### 포함 기능(Features In 매핑)

- **Feature 6:** xAI/Interpretability Suite
- **Feature 7(부분):** LLM 전 단계로서 **ReportBundle 템플릿 + 근거 링크 강제**(문장 생성은 아직 선택)

### 개발 범위(구현 항목)

- ExplainBundle:
    - EEG/PSG: time-segment 중요도 + channel 중요도(최소 2종)
    - tabular: 변수 중요도(최소 1종)
- Error analysis:
    - 실패 케이스 top-N, 조건별 실패 슬라이스(예: 특정 연령대/사이트/품질)
- ReportBundle(템플릿):
    - Methods/Results/Limitations/Next steps 구조
    - 모든 수치/주장에 artifact 링크가 붙도록 강제

### 핵심 산출물(DoD)

- “모델이 왜 그렇게 예측했는지”를 **artifact로 설명** 가능
- LLM 없이도 **표준 연구 리포트(초안 수준)**가 생성됨(수치/근거 링크 포함)

### KPI(공학)

- (E5-1) ExplainBundle 생성 성공률 **≥ 90%**
- (E5-2) Evidence grounding 커버리지: Top-line claim 근거 링크 **100%**
- (E5-3) 보고서 수치 정합성(LLM 전이라도): 표/리포트 값 불일치 **0건**

### KPI(의학)

- (M5-1) xAI 임상 합리성 루브릭: **7/10 이상**, “아티팩트 의존” 판정 시 Fail
- (M5-2) 실패 케이스 분석이 “추가 분석 지시”로 이어질 만큼 구체적이라는 평가(루브릭)
- (M5-3) 과잉 주장(진료 권고/인과 확정) **0건**

### 권장 기간

- **2~4주**

---

## Phase 6 — LLM 통합(설계용 + 문서용 분리, 검증 게이트 내장) (Feature 7)

**Layer1(설계)용 LLM**과 **Layer3(문서화)용 LLM**을 분리하고, 둘 다 **검증 게이트**를 통과해야 산출물로 인정됩니다.

### 6A) LLM for Spec (Layer1)

- 역할: 질문 → TaskSpec/CohortSpec/SplitPlan 초안 생성(“제안”)
- 필수: Spec Validator(룰/코드) + 승인 게이트(사용자)
    
    ### KPI(공학)
    
    - (E6A-1) LLM 생성 Spec의 Validator 통과율 **≥ 70%**(초기) → **≥ 85%**(개선)
    - (E6A-2) Leakage 관련 필수 필드 누락률 **0%**(누락 시 자동 Fail)
    - (E6A-3) “모호한 spec” 자동 플래그율(애매하면 애매하다고 표시): **≥ 95%**
        
        (LLM이 자신감으로 메우지 못하도록)
        
    
    ### KPI(의학)
    
    - (M6A-1) 라벨/endpoint 정의의 임상의 체크리스트 중대 결함 **0**
    - (M6A-2) confounding/한계 섹션 자동 포함률 **100%**(기본 문구라도 반드시 포함)

### 6B) LLM for Report (Layer3)

- 역할: MetricsBundle/ExplainBundle → 서술(Results/Limitations/Next steps)
- 필수: Numeric consistency checker + Evidence grounding checker + Overclaiming checker
    
    ### KPI(공학)
    
    - (E6B-1) 숫자 불일치 **0건**(핵심 지표 기준)
    - (E6B-2) Top-line claim 근거 링크 **100%**
    - (E6B-3) 금지 문구/과잉 주장 탐지 후 잔존율 **0%**
    
    ### KPI(의학)
    
    - (M6B-1) “일반화 결론”은 외부검증이 있을 때만 출력(규칙 준수율 100%)
    - (M6B-2) 리포트 품질 루브릭 **14/20 이상**(필수 항목 미충족 시 Fail)

### 권장 기간(Phase 6 전체)

- **2~4주**
    
    (검증 게이트 구현이 핵심이라, 단순 LLM 연결보다 시간이 더 듭니다.)
    

---

## Phase 7 (v2) — 모델 전략 엔진 확장 + 논문 재현(Replication) + Backbone(Foundation) 고도화

v1에서 **“연구 공정 자동화”**를 먼저 닫고, 그 위에서 “모델 고도화”를 하는 편이 성공 확률이 높습니다.

### 포함 기능(Features In 매핑)

- **Feature 4 전체**(공개 모델 registry, 논문 재현 워크플로, 신규모델 템플릿 확장)
- (선택) 공통 backbone(pretraining) + adapter/head 표준화

### KPI(공학)

- 모델 전략 선택 regret(성능/비용 후회) 감소
- Replication 템플릿 생성 자동화율, 재현 리포트 자동 생성률

### KPI(의학)

- 외부검증 성능 유지율 개선
- xAI 합리성/일관성 개선

### 권장 기간

- Replication 워크플로: **4~8주**
- Backbone(Foundation) v1: 데이터/컴퓨트에 따라 **8~12주+** (병렬로 진행 가능)